{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training/testing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import yaml\n",
    "\n",
    "from common.models import load_model\n",
    "from research.research_config import ResearchConfig\n",
    "from research.src.models import sg_generator\n",
    "from research.src.models import grounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    'atr_net': sg_generator.atr_net,\n",
    "    'motifs_net': sg_generator.motifs_net,\n",
    "    'hgat_net': sg_generator.hgat_net,\n",
    "    'reldn_net': sg_generator.reldn_net,\n",
    "    'uvtranse_net': sg_generator.uvtranse_net,\n",
    "    'vtranse_net': sg_generator.vtranse_net,\n",
    "    'parsing_net': grounder.parsing_net\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseKwargs(argparse.Action):\n",
    "    def __call__(self, parser, namespace, values, option_string=None):\n",
    "        setattr(namespace, self.dest, dict())\n",
    "        for value in values:\n",
    "            key, value = value.split('=')\n",
    "            if value in ['True', 'False']:\n",
    "                value = value == 'True'\n",
    "            else:\n",
    "                try:\n",
    "                    value = float(value)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            getattr(namespace, self.dest)[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    \"\"\"Parse input arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Model to train/test and peculiar parameters\n",
    "    parser.add_argument(\n",
    "        '--model', dest='model', help='Model to train (see main.py)',\n",
    "        type=str, default='lang_spat_net'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--misc_params', nargs='*', action=ParseKwargs, default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--object_classifier', dest='object_classifier',\n",
    "        help='Name of classifier model to use if task is sgcls',\n",
    "        type=str, default='object_classifier'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--teacher', dest='teacher',\n",
    "        help='Name of teacher model to use for distillation',\n",
    "        type=str, default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--teacher_name', dest='teacher_name',\n",
    "        help='Name of teacher net (e.g. visual_net2_predcls_VRD)',\n",
    "        type=str, default=None\n",
    "    )\n",
    "    # Dataset/task parameters\n",
    "    parser.add_argument(\n",
    "        '--dataset', dest='dataset', help='Dataset codename (e.g. VG200)',\n",
    "        type=str, default='VRD'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--task', dest='task',\n",
    "        help='Task to solve, check config.py for supported tasks',\n",
    "        type=str, default='preddet'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--net_name', dest='net_name', help='Name of trained model',\n",
    "        type=str, default=''\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--phrase_recall', dest='phrase_recall',\n",
    "        help='Whether to evaluate phrase recall',\n",
    "        action='store_true'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--test_dataset', dest='test_dataset',\n",
    "        help='Dataset to evaluate on, if different than train dataset',\n",
    "        type=str, default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--test_on_negatives', dest='test_on_negatives',\n",
    "        help='Whether to test on negative labels',\n",
    "        action='store_true'\n",
    "    )\n",
    "    # Specific task parameters: data handling\n",
    "    parser.add_argument(\n",
    "        '--annotations_per_batch', dest='annotations_per_batch',\n",
    "        help='Batch size in terms of annotations (e.g. relationships)',\n",
    "        type=int, default=128\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--not_augment_annotations', dest='not_augment_annotations',\n",
    "        help='Do not augment annotations with box/image distortion',\n",
    "        action='store_true'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--bg_perc', dest='bg_perc',\n",
    "        help='Percentage of background annotations',\n",
    "        type=float, default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--duplicate_rels', dest='duplicate_rels',\n",
    "        help='Whether to filter relations annotated more than once',\n",
    "        action='store_true'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--multiple_preds', dest='multiple_preds',\n",
    "        help='Whether to sample a single predicate per object pair',\n",
    "        action='store_true'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--max_train_samples', dest='max_train_samples',\n",
    "        help='Keep classes at most such many training samples',\n",
    "        type=int, default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_tail_classes', dest='num_tail_classes',\n",
    "        help='Keep such many classes with the fewest training samples',\n",
    "        type=int, default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--use_negative_samples', dest='use_negative_samples',\n",
    "        help='Whether to use extra annotations from negative samples',\n",
    "        action='store_true'\n",
    "    )\n",
    "    # Evaluation parameters\n",
    "    parser.add_argument(\n",
    "        '--compute_accuracy', dest='compute_accuracy',\n",
    "        help='For preddet only, measure accuracy instead of recall',\n",
    "        action='store_true'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--use_merged', dest='use_merged',\n",
    "        help='Evaluate with merged predicate annotations',\n",
    "        action='store_true'\n",
    "    )\n",
    "    # General model parameters\n",
    "    parser.add_argument(\n",
    "        '--is_not_context_projector', dest='is_not_context_projector',\n",
    "        help='Do not treat this projector as a context projector',\n",
    "        action='store_true'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--is_cos_sim_projector', dest='is_cos_sim_projector',\n",
    "        help='Maximize cos. similarity between features and learned weights',\n",
    "        action='store_true'\n",
    "    )\n",
    "    # Specific task parameters: loss function\n",
    "    parser.add_argument(\n",
    "        '--not_use_multi_tasking', dest='not_use_multi_tasking',\n",
    "        help='Do not use multi-tasking to detect \"no interaction\" cases',\n",
    "        action='store_true'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--use_weighted_ce', dest='use_weighted_ce',\n",
    "        help='Use weighted cross-entropy',\n",
    "        action='store_true'\n",
    "    )\n",
    "    # Training parameters\n",
    "    parser.add_argument(\n",
    "        '--batch_size', dest='batch_size',\n",
    "        help='Batch size in terms of images',\n",
    "        type=int, default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--epochs', dest='epochs', help='Number of training epochs',\n",
    "        type=int, default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--learning_rate', dest='learning_rate',\n",
    "        help='Learning rate of classification layers (not backbone)',\n",
    "        type=float, default=0.002\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--weight_decay', dest='weight_decay',\n",
    "        help='Weight decay of optimizer',\n",
    "        type=float, default=None\n",
    "    )\n",
    "    # Learning rate policy\n",
    "    parser.add_argument(\n",
    "        '--apply_dynamic_lr', dest='apply_dynamic_lr',\n",
    "        help='Adapt learning rate so that lr / batch size = const',\n",
    "        action='store_true'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--not_use_early_stopping', dest='not_use_early_stopping',\n",
    "        help='Do not use early stopping learning rate policy',\n",
    "        action='store_true'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--not_restore_on_plateau', dest='not_restore_on_plateau',\n",
    "        help='Do not restore best model on validation plateau',\n",
    "        action='store_true'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--patience', dest='patience',\n",
    "        help='Number of epochs to consider a validation plateu',\n",
    "        type=int, default=1\n",
    "    )\n",
    "    # Other data loader parameters\n",
    "    parser.add_argument(\n",
    "        '--commit', dest='commit',\n",
    "        help='Commit name to tag model',\n",
    "        type=str, default=''\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_workers', dest='num_workers',\n",
    "        help='Number of workers employed by data loader',\n",
    "        type=int, default=2\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--rel_batch_size', dest='rel_batch_size',\n",
    "        help='Number of relations per sub-batch (memory issues)',\n",
    "        type=int, default=128\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--negative_loss', dest='negative_loss',\n",
    "        help='Type of negative loss to use, see _negatives_loss()',\n",
    "        type=str, default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--neg_classes', dest='neg_classes', nargs='+',\n",
    "        help='Classes to implement negative loss, all if not set',\n",
    "        type=int, default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--use_graphl_loss', dest='use_graphl_loss',\n",
    "        help='Whether to use graphical contrastive losses (Zhang 19)',\n",
    "        action='store_true'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--use_consistency_loss', dest='use_consistency_loss',\n",
    "        help='Whether to use consistency loss',\n",
    "        action='store_true'\n",
    "    )\n",
    "    return parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, net_name, teacher, use_consistency_loss, dataset, bg_perc, test_on_negatives):\n",
    "    \"\"\"Train and test a network pipeline.\"\"\"\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Using device:\", device)\n",
    "    \n",
    "    args = parse_args()\n",
    "    model = MODELS[model]\n",
    "    _path = 'prerequisites/'\n",
    "    cfg = ResearchConfig(\n",
    "        net_name=net_name if net_name else model,\n",
    "        phrase_recall=args.phrase_recall, test_dataset=args.test_dataset,\n",
    "        annotations_per_batch=args.annotations_per_batch,\n",
    "        augment_annotations=not args.not_augment_annotations,\n",
    "        compute_accuracy=args.compute_accuracy, use_merged=args.use_merged,\n",
    "        use_multi_tasking=not args.not_use_multi_tasking,\n",
    "        use_weighted_ce=args.use_weighted_ce, batch_size=args.batch_size,\n",
    "        epochs=args.epochs, learning_rate=args.learning_rate,\n",
    "        weight_decay=args.weight_decay, apply_dynamic_lr=args.apply_dynamic_lr,\n",
    "        use_early_stopping=not args.not_use_early_stopping,\n",
    "        restore_on_plateau=not args.not_restore_on_plateau,\n",
    "        patience=args.patience, commit=args.commit,\n",
    "        num_workers=args.num_workers,\n",
    "        use_consistency_loss=use_consistency_loss,\n",
    "        use_graphl_loss=args.use_graphl_loss, misc_params=args.misc_params,\n",
    "        dataset=dataset, task=args.task, bg_perc=bg_perc,\n",
    "        duplicate_rels=args.duplicate_rels,\n",
    "        multiple_preds=args.multiple_preds,\n",
    "        max_train_samples=args.max_train_samples,\n",
    "        num_tail_classes=args.num_tail_classes, \n",
    "        use_negative_samples=args.use_negative_samples,\n",
    "        rel_batch_size=args.rel_batch_size,\n",
    "        neg_classes=args.neg_classes,\n",
    "        negative_loss=args.negative_loss,\n",
    "        prerequisites_path=_path, test_on_negatives=test_on_negatives)\n",
    "    obj_classifier = None\n",
    "    teacher = None\n",
    "    if teacher is not None:\n",
    "        teacher_name = '_'.join([teacher] + cfg.net_name.split('_')[-2:])\n",
    "        if args.teacher_name is not None:\n",
    "            teacher_name = args.teacher_name\n",
    "        teacher = load_model(cfg, teacher,\n",
    "                             path=cfg.prerequisites_path + 'models/' + teacher_name + '/')\n",
    "        for param in teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "    model.train_test(cfg, obj_classifier, teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Please open or close the annotation format according to the following numbered sequence to train the model.\n",
    "    main('parsing_net', 'lang_spat_net', None, False, 'VRD', None, False) # 1. Train grounding network.\n",
    "    #main('atr_net', 'atr_teacher_net', 'parsing_net', True, 'VRD', 0.2, False) # 2. Train the teacher network.\n",
    "    #main('atr_net', 'lang_spat_net', 'atr_teacher_net', False, 'VRD', 0.2, False) # 3. Train a model using the teacher to distill knowledge.\n",
    "    #main('atr_net', 'lang_spat_net', 'atr_teacher_net', False, 'VRD', 0.2, True)  # 4. Use the mP metric to evaluate the trained model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
